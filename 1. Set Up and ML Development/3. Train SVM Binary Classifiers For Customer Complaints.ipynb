{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a283b79e",
   "metadata": {},
   "source": [
    "## Train SVM Binary Classifiers For Customer Complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# External libraries: basic utilities and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# External libraries: data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# External libraries: natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "# External libraries: data science and machine learning\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# External libraries: other utilities\n",
    "import pickle\n",
    "\n",
    "# Setting options and styles\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b93d7",
   "metadata": {},
   "source": [
    "The following block consists of multiple operations centered around text vectorization and normalization using the GloVe word embeddings. Initially, 50-dimensional and 100-dimensional GloVe embeddings are loaded from respective files into two dictionaries. The script also defines two functions, vectorize_text_50 and vectorize_text_100, which vectorize input text using the 50-dimensional and 100-dimensional embeddings, respectively. For texts that don't have corresponding word embeddings, the functions return zero-vectors of matching dimensions. \n",
    "\n",
    "Additionally, a text_normalizer function is introduced that tokenizes a given text, removes specific patterns, converts text to lowercase, and removes tokens that are mere numbers. Lastly, the code loads a CSV file named \"CFPB with Duplicate Marked NEW.csv\" into a DataFrame, prints its shape, and subsequently drops duplicate rows based on the 'dupi_id' column before printing the updated shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 50-dimensional GloVe embeddings into a dictionary\n",
    "glove_6B_50D_txt_file = \"glove_file, something like glove.6B.50d.txt\"\n",
    "embeddings_dict_6B_50D = {}\n",
    "with open(glove_6B_50D_txt_file, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ' '.join(values[:-50]).lower().strip()\n",
    "        vector = np.asarray(values[-50:], \"float32\")\n",
    "        embeddings_dict_6B_50D[word] = vector\n",
    "\n",
    "# Function to vectorize a given text using 50-dimensional GloVe embeddings\n",
    "def vectorize_text_50(text):\n",
    "    vectors = [embeddings_dict_6B_50D.get(word) for word in str(text).split() if word in embeddings_dict_6B_50D]\n",
    "    vectors = [v for v in vectors if v is not None]  # remove any None values\n",
    "    if vectors:\n",
    "        vectorized = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        vectorized = np.zeros(50)  # if there are no vectors, return a zero-vector\n",
    "    return vectorized\n",
    "\n",
    "# Load 100-dimensional GloVe embeddings into a dictionary\n",
    "glove_6B_100D_txt_file = \"glove_file, something like glove.6B.100d.txt\"\n",
    "embeddings_dict_6B_100D = {}\n",
    "with open(glove_6B_100D_txt_file, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ' '.join(values[:-100]).lower().strip()\n",
    "        vector = np.asarray(values[-100:], \"float32\")\n",
    "        embeddings_dict_6B_100D[word] = vector\n",
    "\n",
    "# Function to vectorize a given text using 100-dimensional GloVe embeddings\n",
    "def vectorize_text_100(text):\n",
    "    vectors = [embeddings_dict_6B_100D.get(word) for word in str(text).split() if word in embeddings_dict_6B_100D]\n",
    "    vectors = [v for v in vectors if v is not None]  # remove any None values\n",
    "    if vectors:\n",
    "        vectorized = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        vectorized = np.zeros(100)  # if there are no vectors, return a zero-vector\n",
    "    return vectorized\n",
    "\n",
    "# Function to normalize the text by tokenizing, cleaning up specific patterns and converting to lowercase\n",
    "def text_normalizer(text):\n",
    "    if text:\n",
    "        # Use NLTK RegexpTokenizer for tokenization. \n",
    "        tokenizer = RegexpTokenizer(r'\\b\\w[\\w\\'-]*\\w\\b|\\w')\n",
    "        words = tokenizer.tokenize(text)\n",
    "\n",
    "        # Clean tokens with repeating characters like '666', 'aaa', '!!!!!!'\n",
    "        words = [re.sub(r'(\\w)\\1{2,}', '', word) if re.search(r'(\\w)\\1{2,}', word) else word for word in words]\n",
    "\n",
    "        # Convert to lowercase and remove punctuations.\n",
    "        words = [word.lower().strip() for word in words]\n",
    "\n",
    "        # Remove tokens that are just numbers.\n",
    "        words = ['' if word.isdigit() else word for word in words]\n",
    "\n",
    "        # Join the words back into a single string.\n",
    "        text = ' '.join([word for word in words if word])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load data from CSV for a sanity check\n",
    "complaint_file = \"you complaint file here, should called something like complaints.csv, it hsoul dhave been marked with duplicative complaints\"\n",
    "cfpb_df = pd.read_csv(complaint_file)\n",
    "print(cfpb_df.shape)\n",
    "# Drop duplicates based on 'dupi_id' column\n",
    "cfpb_df = cfpb_df.drop_duplicates(subset='dupi_id')\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ba0b1",
   "metadata": {},
   "source": [
    "The following block aims to address data imbalance by reclassifying and consolidating product and issue categories in a DataFrame named `cfpb_df`. Two dictionaries, `product_mapping` and `issue_dict`, are defined to map the existing product and issue labels to a more condensed set of categories. These mappings are then applied to the 'Product' and 'Issue' columns of the DataFrame, resulting in two new columns: 'combined_product' and 'combined_issue'. To address missing mappings for 'Issue', any unmapped values are retained as they are. Following the reclassification, the code explores the data distribution by visualizing the distribution of narrative lengths (less than 1000 characters) in a histogram. Furthermore, it calculates and displays the normalized distribution of longer narratives (more than 500 characters) by their reclassified product categories, highlighting imbalances in the consolidated categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4074881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to improve unbalance by re-classifiying the data\n",
    "product_mapping = {\n",
    "    \"Credit reporting, credit repair services, or other personal consumer reports\": \"Credit reporting and repair services\",\n",
    "    \"Credit reporting\": \"Credit reporting and repair services\",\n",
    "    \"Credit card or prepaid card\": \"Credit and prepaid cards\",\n",
    "    \"Credit card\": \"Credit and prepaid cards\",\n",
    "    \"Prepaid card\": \"Credit and prepaid cards\",\n",
    "    \"Checking or savings account\": \"Banking services\",\n",
    "    \"Bank account or service\": \"Banking services\",\n",
    "    \"Money transfer, virtual currency, or money service\": \"Money transfer and virtual currency services\",\n",
    "    \"Money transfers\": \"Money transfer and virtual currency services\",\n",
    "    \"Virtual currency\": \"Money transfer and virtual currency services\",\n",
    "    \"Vehicle loan or lease\": \"Loans and leases\",\n",
    "    \"Consumer Loan\": \"Loans and leases\",\n",
    "    \"Payday loan, title loan, or personal loan\": \"Short-term and personal loans\",\n",
    "    \"Payday loan\": \"Short-term and personal loans\",\n",
    "    \"Other financial service\": \"Other financial service\",\n",
    "    \"Mortgage\": \"Mortgage\",\n",
    "    \"Student loan\": \"Student loan\",\n",
    "    \"Debt collection\": \"Debt collection\"\n",
    "}\n",
    "\n",
    "issue_dict = {\n",
    "    \"Improper use of your report\": \"Credit report issues\",\n",
    "    \"Incorrect information on your report\": \"Credit report issues\",\n",
    "    \"Problem with a credit reporting company's investigation into an existing problem\": \"Credit report issues\",\n",
    "    \"Credit reporting company's investigation\": \"Credit report issues\",\n",
    "    \"Improper use of my credit report\": \"Credit report issues\",\n",
    "    \"Incorrect information on credit report\": \"Credit report issues\",\n",
    "    \"Unable to get credit report/credit score\": \"Credit report issues\",\n",
    "\n",
    "    \"Fraud or scam\": \"Fraud and Identity theft issues\",\n",
    "    \"Identity theft / Fraud / Embezzlement\": \"Fraud and Identity theft issues\",\n",
    "    \"Received a loan you didn't apply for\": \"Fraud and Identity theft issues\",\n",
    "\n",
    "    \"Managing an account\": \"Account management issues\",\n",
    "    \"Opening an account\": \"Account management issues\",\n",
    "    \"Closing your account\": \"Account management issues\",\n",
    "    \"Closing an account\": \"Account management issues\",\n",
    "    \"Account opening, closing, or management\": \"Account management issues\",\n",
    "    \"Managing, opening, or closing your mobile wallet account\": \"Account management issues\",\n",
    "    \"Managing, opening, or closing account\": \"Account management issues\",\n",
    "\n",
    "    \"Attempts to collect debt not owed\": \"Unjustified debt collection attempts\",\n",
    "    \"Cont'd attempts collect debt not owed\": \"Unjustified debt collection attempts\",\n",
    "\n",
    "    \"Problem with a purchase shown on your statement\": \"Transaction issues\",\n",
    "    \"Problem with a purchase or transfer\": \"Transaction issues\",\n",
    "    \"Other transaction problem\": \"Transaction issues\",\n",
    "    \"Unauthorized transactions or other transaction problem\": \"Transaction issues\",\n",
    "    \"Other transaction issues\": \"Transaction issues\",\n",
    "    \"Transaction issue\": \"Transaction issues\",\n",
    "    \"Unauthorized transactions/trans. issues\": \"Transaction issues\",\n",
    "\n",
    "    \"Struggling to pay your bill\": \"Payment struggles\",\n",
    "    \"Struggling to pay your loan\": \"Payment struggles\",\n",
    "    \"Struggling to repay your loan\": \"Payment struggles\",\n",
    "    \"Can't repay my loan\": \"Payment struggles\",\n",
    "    \"Problems when you are unable to pay\": \"Payment struggles\",\n",
    "\n",
    "    \"Problem with fraud alerts or security freezes\": \"Fraud alerts and identity protection issues\",\n",
    "    \"Identity theft protection or other monitoring services\": \"Fraud alerts and identity protection issues\",\n",
    "    \"Credit monitoring or identity theft protection services\": \"Fraud alerts and identity protection issues\",\n",
    "    \"Credit monitoring or identity protection\": \"Fraud alerts and identity protection issues\",\n",
    "\n",
    "    \"Took or threatened to take negative or legal action\": \"Improper actions or threats\",\n",
    "    \"Threatened to contact someone or share information improperly\": \"Improper actions or threats\",\n",
    "    \"Taking/threatening an illegal action\": \"Improper actions or threats\",\n",
    "    \"Improper contact or sharing of info\": \"Improper actions or threats\",\n",
    "\n",
    "    \"Fees or interest\": \"Fee and interest issues\",\n",
    "    \"Charged fees or interest you didn't expect\": \"Fee and interest issues\",\n",
    "    \"Unexpected or other fees\": \"Fee and interest issues\",\n",
    "    \"Fees\": \"Fee and interest issues\",\n",
    "    \"Cash advance fee\": \"Fee and interest issues\",\n",
    "    \"Overlimit fee\": \"Fee and interest issues\",\n",
    "    \"Balance transfer fee\": \"Fee and interest issues\",\n",
    "\n",
    "    \"Getting a credit card\": \"Credit and loan acquisition issues\",\n",
    "    \"Trouble using your card\": \"Credit and loan acquisition issues\",\n",
    "    \"Trouble using the card\": \"Credit and loan acquisition issues\",\n",
    "    \"Getting a line of credit\": \"Credit and loan acquisition issues\",\n",
    "    \"Shopping for a loan or lease\": \"Credit and loan acquisition issues\",\n",
    "    \"Shopping for a line of credit\": \"Credit and loan acquisition issues\",\n",
    "\n",
    "    \"Problem with customer service\": \"Customer service issues\",\n",
    "    \"Customer service / Customer relations\": \"Customer service issues\",\n",
    "    \"Customer service/Customer relations\": \"Customer service issues\",\n",
    "    \n",
    "    \"Can't contact lender or servicer\": \"Communication issues\",\n",
    "    \"Can't contact lender\": \"Communication issues\",\n",
    "    \"Communication tactics\": \"Communication issues\",\n",
    "    \n",
    "    \"Problem with the payoff process at the end of the loan\": \"Loan issues\",\n",
    "    \"Problems at the end of the loan or lease\": \"Loan issues\",\n",
    "    \"Managing the loan or lease\": \"Loan issues\",\n",
    "    \"Loan payment wasn't credited to your account\": \"Loan issues\",\n",
    "    \"Loan servicing, payments, escrow account\": \"Loan issues\",\n",
    "    \"Problem when making payments\": \"Loan issues\",\n",
    "    \"Making/receiving payments, sending money\": \"Loan issues\",\n",
    "    \"Managing the line of credit\": \"Loan issues\",\n",
    "    \"Loan modification,collection,foreclosure\": \"Loan issues\",\n",
    "    \n",
    "    \"Advertising and marketing, including promotional offers\": \"Advertising and marketing issues\",\n",
    "    \"Advertising\": \"Advertising and marketing issues\",\n",
    "    \"Advertising and marketing\": \"Advertising and marketing issues\",\n",
    "    \"Confusing or misleading advertising or marketing\": \"Advertising and marketing issues\",\n",
    "    \"Advertising, marketing or disclosures\": \"Advertising and marketing issues\",\n",
    "    \n",
    "    \"Problem with a lender or other company charging your account\": \"Lender issues\",\n",
    "    \"Dealing with your lender or servicer\": \"Lender issues\",\n",
    "    \"Can't stop withdrawals from your bank account\": \"Lender issues\",\n",
    "    \"Money was taken from your bank account on the wrong day or for the wrong amount\": \"Lender issues\",\n",
    "    \"Was approved for a loan, but didn't receive the money\": \"Lender issues\",\n",
    "    \"Applied for loan/did not receive money\": \"Lender issues\",\n",
    "    \"Was approved for a loan, but didn't receive money\": \"Lender issues\",\n",
    "    \n",
    "    \"Problem with additional add-on products or services\": \"Product and service issues\",\n",
    "    \"Other service problem\": \"Product and service issues\",\n",
    "    \"Other service issues\": \"Product and service issues\",\n",
    "    \n",
    "    \"Problem with cash advance\": \"Transaction and payment issues\",\n",
    "    \"Charged bank acct wrong day or amt\": \"Transaction and payment issues\",\n",
    "    \"Payment to acct not credited\": \"Transaction and payment issues\",\n",
    "    \"Payoff process\": \"Transaction and payment issues\",\n",
    "    \"Cash advance\": \"Transaction and payment issues\",\n",
    "    \n",
    "    \"False statements or representation\": \"False statements and representation issues\",\n",
    "    \"Confusing or missing disclosures\": \"False statements and representation issues\",\n",
    "    \"Disclosure verification of debt\": \"False statements and representation issues\",\n",
    "    \"Incorrect/missing disclosures or info\": \"False statements and representation issues\",\n",
    "    \"Disclosures\": \"False statements and representation issues\",\n",
    "    \"Incorrect exchange rate\": \"False statements and representation issues\",\n",
    "    \n",
    "    \"Applying for a mortgage or refinancing an existing mortgage\": \"Mortgage issues\",\n",
    "    \"Struggling to pay mortgage\": \"Mortgage issues\",\n",
    "    \"Closing on a mortgage\": \"Mortgage issues\",\n",
    "    \"Application, originator, mortgage broker\": \"Mortgage issues\",\n",
    "    \n",
    "    \"Credit limit changed\": \"Credit issues\",\n",
    "    \"Credit decision / Underwriting\": \"Credit issues\",\n",
    "    \"Credit card protection / Debt protection\": \"Credit issues\",\n",
    "    \"Rewards\": \"Credit issues\",\n",
    "    \"Credit determination\": \"Credit issues\",\n",
    "    \"Credit line increase/decrease\": \"Credit issues\",\n",
    "    \"APR or interest rate\": \"Credit issues\",\n",
    "    \n",
    "    \"Vehicle was repossessed or sold the vehicle\": \"Vehicle issues\",\n",
    "    \"Vehicle was damaged or destroyed the vehicle\": \"Vehicle issues\",\n",
    "    \"Lender repossessed or sold the vehicle\": \"Vehicle issues\",\n",
    "    \"Lender damaged or destroyed vehicle\": \"Vehicle issues\",\n",
    "    \"Property was sold\": \"Vehicle issues\",\n",
    "    \"Property was damaged or destroyed property\": \"Vehicle issues\",\n",
    "    \"Lender sold the property\": \"Vehicle issues\",\n",
    "    \"Lender damaged or destroyed property\": \"Vehicle issues\",\n",
    "    \n",
    "    \"Bankruptcy\": \"Bankruptcy issues\",\n",
    "    \"Balance transfer\": \"Balance transfer issues\",\n",
    "    \"Using a debit or ATM card\": \"Debit and ATM card issues\",\n",
    "    \n",
    "    \"Privacy\": \"Privacy issues\",\n",
    "    \n",
    "    \"Forbearance / Workout plans\": \"Workout plan issues\",\n",
    "    \"Sale of account\": \"Account sale issues\",\n",
    "    \n",
    "    \"Adding money\": \"Money adding issues\",\n",
    "    \"Problem adding money\": \"Money adding issues\",\n",
    "    \n",
    "    \"Delinquent account\": \"Account delinquency issues\",\n",
    "    \n",
    "    \"Application processing delay\": \"Application processing issues\",\n",
    "    \n",
    "    \"Arbitration\": \"Arbitration issues\",\n",
    "    \n",
    "    \"Convenience checks\": \"Check issues\",\n",
    "    \"Lost or stolen check\": \"Check issues\",\n",
    "    \n",
    "    \"Overdraft, savings, or rewards features\": \"Overdraft, savings, or rewards issues\",\n",
    "    \"Overdraft, savings or rewards features\": \"Overdraft, savings, or rewards issues\",\n",
    "    \n",
    "    \"Unexpected/Other fees\": \"Unexpected fees issues\",\n",
    "    \"Balance transfer fee\": \"Unexpected fees issues\",\n",
    "    \"Excessive fees\": \"Unexpected fees issues\",\n",
    "    \n",
    "    \"Other\": \"Other issues\"\n",
    "}\n",
    "\n",
    "\n",
    "# apply the mapping to the 'Product' and 'Issue\" column\n",
    "cfpb_df['combined_product'] = cfpb_df['Product'].map(product_mapping)\n",
    "cfpb_df['combined_issue'] = cfpb_df['Issue'].map(issue_dict).fillna(cfpb_df['Issue'])\n",
    "\n",
    "# check the distribution of narrative length by character:\n",
    "cfpb_df[cfpb_df['narr_len']<1000]['narr_len'].plot(kind='hist', bins=100)\n",
    "\n",
    "# check the distribution of longer narrative length by products, even by combined products, the imbalance is critical\n",
    "cfpb_df[(cfpb_df['narr_len']>500)].combined_product.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0a788",
   "metadata": {},
   "source": [
    "The code defines three primary functions aimed at supporting SVM-based text classification tasks:\n",
    "\n",
    "1. `plot_proba_distribution`: This function visualizes the distribution of predicted probabilities across multiple classes.\n",
    "2. `grid_search_svm`: For each unique value in a specified column of the dataframe, this function constructs a binary classification problem, balances the classes, then performs a grid search over specified hyperparameters of an SVM to identify optimal parameters. It iteratively trains and evaluates SVM models on resampled data of varying sizes, printing out performance metrics and best parameters for each iteration.\n",
    "3. `create_svm_dict`: This function constructs a dictionary of trained SVM models for each unique value in a given dataframe column. It skips training for classes with significant imbalances. For each category, the narratives are vectorized, the dataset is split and balanced, and an SVM model with specified hyperparameters is trained. The trained models are stored in the dictionary, keyed by the unique values of the column.\n",
    "\n",
    "In essence, the code facilitates the exploration, tuning, and application of SVM classifiers for binary text classification tasks on a per-category basis in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the distribution of predicted probabilities\n",
    "def plot_proba_distribution(y_pred_proba, classes):\n",
    "    # Loop through each class and plot its probability distribution\n",
    "    for idx, _class in enumerate(classes):\n",
    "        sns.kdeplot(y_pred_proba[:, idx], label=_class)\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Probability distributions of classes')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to perform a grid search on SVM for a given column in the dataframe\n",
    "def print_grid_search_svm_results(df, column_name):\n",
    "    # Loop through each unique value in the column\n",
    "    for unique_value in df[column_name].unique():\n",
    "        try:\n",
    "            print(f\"now training binary classification model for {unique_value}\")\n",
    "            # create binary target column\n",
    "            df['is_'+ unique_value] = df[column_name].apply(lambda x: 1 if x==unique_value else 0)\n",
    "            print(df['is_'+ unique_value].value_counts())\n",
    "\n",
    "            # vectorize the clean narrative using the function defined previously\n",
    "            df['glove_50_features'] = df['clean_narr'].apply(vectorize_text_100)\n",
    "\n",
    "            # Create features and target variables\n",
    "            X = list(df['glove_50_features'])\n",
    "            y = df['is_'+ unique_value]\n",
    "\n",
    "            # Split the data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "            # Balance the dataset using RandomUnderSampling\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).tolist(), y_train)\n",
    "            print(y_train_resampled.value_counts())\n",
    "\n",
    "            # Perform grid search for multiple data sizes\n",
    "            for data_size in [5000, 10000, 15000, 20000]:\n",
    "                # Randomly sample data\n",
    "                sample_idx = np.random.choice(len(X_train_resampled), data_size, replace=False)\n",
    "                X_train_resampled_sampled = [X_train_resampled[i] for i in sample_idx]\n",
    "                y_train_resampled_sampled = [y_train_resampled[i] for i in sample_idx]\n",
    "\n",
    "                # Define grid parameters\n",
    "                param_grid = {'C': [50, 100, 250], 'gamma': [0.05, 0.15, 0.25, 0.5, 0.75, 0.99], 'kernel': ['rbf']}\n",
    "\n",
    "                # initialize SVM and grid search\n",
    "                svm = SVC(class_weight='balanced')\n",
    "                svm.classes_ = np.array([0, 1])\n",
    "                grid = GridSearchCV(svm, param_grid, cv=5, scoring='f1', verbose=4, n_jobs=-1)\n",
    "                grid.fit(np.array(X_train_resampled_sampled).tolist(), y_train_resampled_sampled)\n",
    "\n",
    "                # Evaluate model\n",
    "                y_pred = grid.predict(np.array(X_test).tolist())\n",
    "                print(f1_score(y_test, y_pred, average='weighted'))\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                print(\"The best parameters:\", grid.best_params_)\n",
    "                print(\"Data size:\", data_size)\n",
    "\n",
    "                # Print grid search results\n",
    "                results = grid.cv_results_\n",
    "                for mean_test_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "                    print(params, \"has a score of\", mean_test_score)\n",
    "                print(\"-----------------------------------------------------------------------\")\n",
    "            print(\"**************************************************************************\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Function to create a dictionary of SVM models for each unique value in a given column, the best parameters will be used in this function\n",
    "def create_svm_dict(df, column_name):\n",
    "    ml_dict={}\n",
    "    for unique_value in df[column_name].unique():\n",
    "        try:\n",
    "            print(f\"now training binary classification model for {unique_value}\")\n",
    "            # create binary target column\n",
    "            df['is_'+ unique_value] = df[column_name].apply(lambda x: 1 if x==unique_value else 0)\n",
    "            \n",
    "            # Skip training for highly imbalanced classes\n",
    "            value_counts = df['is_'+ unique_value].value_counts()\n",
    "            if min(value_counts)/max(value_counts) < 0.01:\n",
    "                print(f\"Skipping training binary classification model for {unique_value} due to class imbalance.\")\n",
    "                continue\n",
    "\n",
    "            # Vectorize the clean narrative\n",
    "            df['glove_50_features'] = df['clean_narr'].apply(vectorize_text_100)\n",
    "\n",
    "            # Create features and target variables\n",
    "            X = list(df['glove_50_features'])\n",
    "            y = df['is_'+ unique_value]\n",
    "\n",
    "            # Split the data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "            # Balance the dataset using RandomUnderSampling\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            X_train_resampled, y_train_resampled = rus.fit_resample(np.array(X_train).tolist(), y_train)\n",
    "            print(y_train_resampled.value_counts())\n",
    "\n",
    "            # Randomly sample data\n",
    "            sample_idx = np.random.choice(len(X_train_resampled), 20000, replace=False)\n",
    "            X_train_resampled_sampled = [X_train_resampled[i] for i in sample_idx]\n",
    "            y_train_resampled_sampled = [y_train_resampled[i] for i in sample_idx]\n",
    "\n",
    "            # Initialize SVM with specified parameters and fit the model, this is the best \n",
    "            svm = SVC(class_weight='balanced', C=50, gamma=0.1, kernel='rbf', probability=True)\n",
    "            svm.classes_ = np.array([0, 1])\n",
    "            svm.fit(np.array(X_train_resampled_sampled).tolist(), y_train_resampled_sampled)\n",
    "\n",
    "            # Evaluate model\n",
    "            y_pred = svm.predict(np.array(X_test).tolist())\n",
    "            print(f1_score(y_test, y_pred, average='weighted'))\n",
    "            print(classification_report(y_test, y_pred))\n",
    "\n",
    "            # Add trained SVM to the dictionary\n",
    "            ml_dict[unique_value] = svm\n",
    "            print(\"-----------------------------------------------------------------------\")\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    print(\"**************************************************************************\")\n",
    "    return ml_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b9d75",
   "metadata": {},
   "source": [
    "To observe and pick the best parameters for model training, run the following code. This will take some time and print a lot of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Product did not significantly changed the distribution of products,so we will train models using combined products\n",
    "product_svms = print_grid_search_svm_results(cfpb_df.copy(), \"combined_product\")\n",
    "product_svms_long = print_grid_search_svm_results(cfpb_df[(cfpb_df['narr_len']>500)].copy(), \"combined_product\")\n",
    "issue_svms = print_grid_search_svm_results(cfpb_df.copy(), \"Issue\")\n",
    "issue_svms_long = print_grid_search_svm_results(cfpb_df[(cfpb_df['narr_len']>500)].copy(), \"Issue\")\n",
    "combined_issue_svms = print_grid_search_svm_results(cfpb_df.copy(), \"combined_issue\")\n",
    "combined_issue_svms_long = print_grid_search_svm_results(cfpb_df[(cfpb_df['narr_len']>500)].copy(), \"combined_issue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b1bd4",
   "metadata": {},
   "source": [
    "The following code is dedicated to training multiple SVM classifiers on different categories from the cfpb_df dataframe, especially emphasizing handling class imbalances. It focuses on two primary categories: 'combined_product' and 'Issue', and their respective longer narrative versions, where narratives exceed 500 characters in length. The objective is to construct classifiers for both the original and combined versions of these categories. \n",
    "\n",
    "Despite the non-significant change in product distribution after combining (due to imbalance challenges), models for 'combined_product' are still trained. The models for combined issues are given preference due to their marginally higher accuracy and significant recall. Given the severe imbalance, where ratios can reach 100:1, the strategy leans towards accepting some false positives as it's deemed better than missing out on detecting any minority cases. \n",
    "\n",
    "Once these models are trained, considering the challenges posed by imbalances, they are serialized and saved as .pkl files for potential future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3568535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training multiple SVMs and storing them in dictionaries\n",
    "\n",
    "# Combined Product did not significantly changed the distribution of products,so we will train models using combined products\n",
    "product_svms = create_svm_dict(cfpb_df.copy(), \"combined_product\")\n",
    "product_svms_long = create_svm_dict(cfpb_df[(cfpb_df['narr_len']>500)].copy(), \"combined_product\")\n",
    "\n",
    "\n",
    "# We observed that combined issues has slightly higer accuracy rate while having high recalls\n",
    "# Considering the imbalance ratio is around 100:1, we believe that some fales positives/miss identifying as positive is better than miss identify any minotity cases\n",
    "issue_svms = create_svm_dict(cfpb_df.copy(), \"Issue\")\n",
    "issue_svms_long = create_svm_dict(cfpb_df[(cfpb_df['narr_len']>500)].copy(), \"Issue\")\n",
    "combined_issue_svms = create_svm_dict(cfpb_df.copy(), \"combined_issue\")\n",
    "combined_issue_svms_long = create_svm_dict(cfpb_df[(cfpb_df['narr_len']>500)].copy(), \"combined_issue\")\n",
    "\n",
    "\n",
    "# Save each model\n",
    "with open(\"_product_svm_models_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(product_svms, file)\n",
    "with open(\"_product_svm_long_models_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(product_svms_long, file) \n",
    "with open(\"_combined_issue_svm_models_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(combined_issue_svms, file)\n",
    "with open(\"_combined_issue_svm_long_models_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(combined_issue_svms_long, file)  \n",
    "with open(\"_issue_svm_models_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(issue_svms, file)\n",
    "with open(\"_issue_svm_long_models_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(issue_svms_long, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd068a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
