{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422f2676",
   "metadata": {},
   "source": [
    "## Narrative Similarity Analysis on CFPB Complaints Using GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58653a7a",
   "metadata": {},
   "source": [
    "### This script provides a comprehensive workflow for processing, analyzing, and identifying potential duplicates in consumer complaint narratives sourced from the Consumer Financial Protection Bureau (CFPB). Leveraging the power of GloVe word embeddings, the script first loads the embeddings to convert narratives into vector representations. It then introduces functions to normalize and vectorize the narratives. After preprocessing, the code identifies narratives that are potentially similar based on their vectorized representations and timeframes of submission. Ultimately, duplicates are marked, and the processed dataset, complete with identified duplicate narratives, is saved for further analysis. This approach aids in reducing redundancy and ensures a cleaner dataset for subsequent investigations or reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eecd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party Libraries for Data Manipulation and Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "# Visualization and Display\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning and Embeddings\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Miscellaneous\n",
    "import string\n",
    "\n",
    "# Setting IPython display options for better visualization\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabcc74",
   "metadata": {},
   "source": [
    "#### 1. Loading Embeddings: The GloVe (Global Vectors for Word Representation) embeddings of size 50 (something like glove.6B.50d.txt) are loaded into a dictionary (embeddings_dict_6B_50D). These embeddings are essentially vector representations of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9782a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GloVe word embeddings into a dictionary\n",
    "glove_txt_file = \"glove_file, something like glove.6B.50d.txt\"\n",
    "embeddings_dict_6B_50D = {}\n",
    "with open(glove_txt_file, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ' '.join(values[:-50]).lower().strip()\n",
    "        vector = np.asarray(values[-50:], \"float32\")\n",
    "        embeddings_dict_6B_50D[word] = vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1864eb",
   "metadata": {},
   "source": [
    "#### 2. Text Vectorization: A function vectorize_text is defined to convert a given text into a vector form using the aforementioned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aac82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into its vector representation\n",
    "def vectorize_text(text):\n",
    "    vectors = [embeddings_dict_6B_50D.get(word) for word in str(text).split() if word in embeddings_dict_6B_50D]\n",
    "    vectors = [v for v in vectors if v is not None]  # remove any None values\n",
    "    if vectors:\n",
    "        vectorized = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        vectorized = np.zeros(50)  # if there are no vectors, return a zero-vector\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53a7f5",
   "metadata": {},
   "source": [
    "#### 3. Text Normalization: The text_normalizer function is responsible for cleaning and pre-processing text. This involves tokenization, removing redundant characters, converting to lowercase, and removing punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f83553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and clean the given text\n",
    "def text_normalizer(text):\n",
    "    if text:\n",
    "        # Tokenization while retaining words with apostrophes\n",
    "        tokenizer = RegexpTokenizer(r'\\b\\w[\\w\\'-]*\\w\\b|\\w')\n",
    "        words = tokenizer.tokenize(text)\n",
    "        \n",
    "        # Remove tokens with repeating characters\n",
    "        words = [re.sub(r'(\\w)\\1{2,}', '', word) if re.search(r'(\\w)\\1{2,}', word) else word for word in words]\n",
    "        \n",
    "        # Convert to lowercase and remove punctuations\n",
    "        words = [word.lower().strip() for word in words]\n",
    "        \n",
    "        # Substitute tokens that are just numbers with empty strings\n",
    "        words = ['' if word.isdigit() else word for word in words]\n",
    "        \n",
    "        # Merge words into a single string\n",
    "        text = ' '.join([word for word in words if word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5bd99",
   "metadata": {},
   "source": [
    "#### 4. Data Preprocessing: The CFPB dataset is loaded into a dataframe, and various transformations are applied. These transformations include:\n",
    "* Removing rows with NaN values in the 'Consumer complaint narrative' column.\n",
    "* Converting the 'Date received' column into datetime format.\n",
    "* Computing the length of each narrative.\n",
    "* Calculating the number of days since the complaint was received.\n",
    "* Applying the text normalization function to the 'Consumer complaint narrative' column.\n",
    "* Vectorizing the first 500 characters of each normalized narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "complaint_file = \"you complaint file here, should called complaints.csv\"\n",
    "cfpb_df = pd.read_csv(complaint_file)\n",
    "\n",
    "# Data preprocessing\n",
    "print(\"Before dropping nan narrative: \", len(cfpb_df))\n",
    "cfpb_df.dropna(subset=['Consumer complaint narrative'], inplace=True)\n",
    "cfpb_df['Date received'] = pd.to_datetime(cfpb_df['Date received'])\n",
    "cfpb_df['narr_len'] = cfpb_df['Consumer complaint narrative'].apply(lambda x:len(str(x)))\n",
    "cfpb_df['days_to_today'] = (datetime.now().date() - cfpb_df['Date received'].dt.date).dt.days\n",
    "cfpb_df['narr_len'] = cfpb_df['narr_len'].astype(int)\n",
    "cfpb_df['days_to_today'] = cfpb_df['days_to_today'].astype(int)\n",
    "cfpb_df['clean_narr'] = cfpb_df['Consumer complaint narrative'].apply(text_normalizer)\n",
    "cfpb_df['narr_head_vec'] = cfpb_df['clean_narr'].apply(lambda x: vectorize_text(x[:500]))\n",
    "print(\"After dropping nan narrative: \", len(cfpb_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27df9e",
   "metadata": {},
   "source": [
    "#### 5. Identifying Duplicate Narratives: The function find_duplicate_narr is aimed at identifying potential duplicate narratives by analyzing the vector representation of their content. This is achieved by measuring the Euclidean distance between vector representations of narratives. If the distance is below a specified threshold, narratives are marked as duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8af0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a smaller version of the dataframe for processing\n",
    "small_cfpb_df = cfpb_df[['State', 'ZIP code','Complaint ID','narr_len', 'days_to_today','narr_head_vec']].copy()\n",
    "small_cfpb_df[['State', 'ZIP code']] = small_cfpb_df[['State', 'ZIP code']].fillna('')\n",
    "\n",
    "# Function to identify duplicate narratives by checking vector similarity\n",
    "def find_duplicate_narr(df):\n",
    "    small_df = df[['Complaint ID','narr_len', 'days_to_today', 'narr_head_vec']].copy()\n",
    "    def find_dupi_in_small_df(row_narr_len, row_to_day, row_narr_head_vec, small_df):\n",
    "        tmp_df = small_df.query(\"narr_len <= @row_narr_len*1.2 & narr_len >= @row_narr_len*0.8 & days_to_today <= @row_to_day+5 & days_to_today >= @row_to_day-5\").copy()    \n",
    "        tmp_df['eclidean_dist'] = tmp_df['narr_head_vec'].apply(lambda x: np.linalg.norm(x - row_narr_head_vec))\n",
    "        dupli_df = tmp_df[tmp_df['eclidean_dist']<0.25]\n",
    "        dupli_id_list = sorted(dupli_df['Complaint ID'].to_list())\n",
    "        return dupli_id_list\n",
    "    df['dupi_id'] = small_df.apply(lambda row: find_dupi_in_small_df(row['narr_len'], int(row['days_to_today']), row['narr_head_vec'], small_df),axis=1)\n",
    "    return df\n",
    "\n",
    "# Applying the duplicate finder function to the dataframe\n",
    "small_cfpb_df = small_cfpb_df.groupby(['State', 'ZIP code']).apply(func=find_duplicate_narr)\n",
    "small_cfpb_df['dupi_len'] = small_cfpb_df['dupi_id'].apply(lambda x: len(x))\n",
    "small_cfpb_df['dupi_id'] = small_cfpb_df['dupi_id'].apply(lambda x: \";\".join([str(y) for y in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe557337",
   "metadata": {},
   "source": [
    "#### 6. Merging and Saving Data: The identified duplicates are merged back into the original dataframe, and the processed dataset with marked duplicates is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e85255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging identified duplicates back to the original dataframe\n",
    "merged_df = cfpb_df.merge(small_cfpb_df[['Complaint ID', 'dupi_id', 'dupi_len']], on='Complaint ID', how='left').drop(['narr_head_vec'], axis=1)\n",
    "\n",
    "# Saving the processed dataset with marked duplicates\n",
    "save_file_name = \"where you want to store\"\n",
    "merged_df.to_csv('save_file_name', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
